#chapter4 深层神经网络  
##4.1 深度学习与深层神经网络  
深度学习：一定通过多层非线性变换对高度复杂性数据建模算法的合集。  
深度神经网络是实现“多层非线性变换”的常用方法，所以实际中，深度学习就是深层神经网络的代名词  
深度学习两个特性： 多层，非线性
###4.1.1 线性模型的局限性
    线性模型：
    线性代数，以及向前传播计算公式，矩阵的乘法……    
    只通过线性变换，任意层全连接神经网络和单层神经网络模型表达能力没有差别（矩阵乘法），这就是线性模型最大的局限性，所以深度学习强调非线性！
    在线性可分问题中，线性模型就能很好区分不同颜色的点。因为线性模型就能解决线性可分问题，所以在深度学习的定义中特意强调它的目的为解决更 加复杂的问题。 所谓复杂问题，至少是无法通过直线（或者高维空间的平面）划分的    
###4.1.2激活函数实现去线性化
激活函数是如何工作
神经元结构的输出为所有输入的加 权和，这导致整个神经网络是一个线性模型。
如果将每一个神经元（也就是神经网络中的 节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了.这个非线性函数就是激活函数
加入激活函数和偏置项之后
中增加了 偏置项（bias），偏置项是神经网络中非常常用的一种结构。 第二个改变就是每个节点的取 值不再是单纯的加权和。 每个节点的输出在加权和的基础上还做了一个非线性变换。 

###4.1.3 多层网络解决异或运
解深度学习的另外一个重要性质一一多层变换
一个能够模拟异或运算的数据集
##4.2 损失函数定义
神经网络模型的效果以及 优化的目标是通过损失函数（ loss function）来定义
###4.2.1 经典损失函数
分类问题和回归问题是监督学习的两大种类
分类问题希望解决的是将不同的样本分到事先定义好的类别中
个零件是否合格的问题就是一个二分类问题。
手写体数字识别问题可以被归纳成一个十分类问题。
虽然设置多个阔值在理论上是可能的，但在解决实际问题 的过程中一般不会这么处理。 
络解决多分类问题最常用的方法是设置 n 个输出节点，其中 n 为类别的个 数
判断一个输出 向量和期望的向盘有多接近呢？交叉蛐（cross entropy）是常用的评判方法之一。交叉：脑刻 画了两个概率分布之间的距离， 它是分类问题中使用 比较广的一种损失函数。 
交叉姻是一个信息论中的概念，它原本是用来估算平均编码长度的
两个概率分布p 和 q， 通过 q 来表示p 的交叉煽为H(p,q),两个概率分布之间的距离
利用Softmax回归将神经网络向前传播得到的结果也变成概率分布。Softmax回归可以作为一个学习算法来优化分类结果，，但在 TensorFlow 中， s。如nax 回归的参数被去掉了，它只是一层额外的处理层，将神经网络的输出变成一个概率分布
原始神经网络的输出被用作置信度来生成新的输出，而新的 输出满足概率分布的所有要求。这个新的输出可以理解为经过神经网络的推导， 一个样例 为不同类别的概率分别是多大

深层神经网络实际上有组合特征提取的功能
实现交叉熵
    tf.clip_by_ value 函数可以将一个张量中的 数值限制在一个范围之内  
    第二个运算是 tf.log 函数，这个函数完成了对张量中所有元素依次求对数 的功能  
    是乘法，在实现交叉熵的代码中直接将两个矩阵通过“＊”操作相乘。这个 操作不是矩阵乘法，而是元素之间直接相乘。矩阵乘法需要使用 tf.matmul 函数来完成。  
        vl*v2 的结果是每个位置上对应元素的乘积
这三个运算完成了对于每一个样例中的每一个类别交叉熵p(x)logq(x）的计 算。这三步计算得到的结果是一个 n × m 的二维矩阵，其中n为一个batch中样例的数量， m 为分类的类别数量。根据交叉娟的公式，应该将每行中的 m 个结果相加得到所有样例的 交叉熵，然后再对这n行取平均得到一个batch的平均交叉熵。但因为分类问题的类别数量是不变的，所以可以直接对整个矩阵做平均而并不改变计算结果的意义。这样的方式可以使整个程序更加简洁。以下代码简单展示了 tf.reduce mean函数的使用方法。 
tf.reduce mean 

因为交叉熵一般会与softmax 回归一起使用，所以 TensorFlow 对这两个功能进行了统一封装，并提供了tf.nn.softmax_cross_entropy_with_logits 函数
在只有一个正确答案的分类问题中，TensorFlow 提供了tf.nn.sparse_softmax_cross_entropy_with_logits

#####回归问题：与分类问题不同，回归问题解决的是对具体数值的预
解决回归问题的神经网络一般只有一个输出节点，这个节点的输出值就是预测值。对于回归 问题，最常用的损失函数是均方误差 MSE, meansquared error 
“－”也是两个矩阵中对应元素的减法。

###4.2.2自定义损失函数
何通过自定义损失函数的方法，使得神经网络优化的结果更加接近实际问题的需求。
，如果预测多了（预测值比真实销量大），商家损失的是生产商品的 成本；而如果预测少了（预测值比真实销量小〉，损失的则是商品的利润。因为一般商品的 成本和商品的利润不会严格相等，
所以使用4.2.1 节中介绍的均方误差损失函数就不能够很好地最大化销售利润 
。注意损失函数定义的是损失，所以要将利润最大化，定义的损失函数应该 刻画成本或者代价。 
##4.3神经网络损失算法
##4.4神经网络进一步优化
###4.4.1学习率的设置
###4.4.2过拟合问题
