#chapter4 深层神经网络  
##4.1 深度学习与深层神经网络  
深度学习：一定通过多层非线性变换对高度复杂性数据建模算法的合集。  
深度神经网络是实现“多层非线性变换”的常用方法，所以实际中，深度学习就是深层神经网络的代名词  
深度学习两个特性： 多层，非线性
###4.1.1 线性模型的局限性
    线性模型：
    线性代数，以及向前传播计算公式，矩阵的乘法……    
    只通过线性变换，任意层全连接神经网络和单层神经网络模型表达能力没有差别（矩阵乘法），这就是线性模型最大的局限性，所以深度学习强调非线性
    在线性可分问题中，线性模型就能很好区分不同颜色的点。因为线性模型就能解决线性可分问题，所以在深度学习的定义中特意强调它的目的为解决更加复杂的问题。 所谓复杂问题，至少是无法通过直线（或者高维空间的平面）划分的    
###4.1.2激活函数实现去线性化
激活函数是如何工作
神经元结构的输出为所有输入的加权和，这导致整个神经网络是一个线性模型。
如果将每一个神经元（也就是神经网络中的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了，这个非线性函数就是激活函数
加入激活函数和偏置项之后
中增加了 偏置项（bias），偏置项是神经网络中非常常用的一种结构。 第二个改变就是每个节点的取 值不再是单纯的加权和。 每个节点的输出在加权和的基础上还做了一个非线性变换。 
目前 TensorFlow 提供了7种不同的非线性激活函数，tf.nn.relu、 tf.sigmoid 和 tf.tanh 是其中比较常用的几个。TensorFlow也支持使用自己定义的激活函数
```
a= tf.nn.relu (tf.matmul(x, w1) + biases1)
y = tf.nn.relu(tf.matmul(a, w2) + biases2)
```
###4.1.3 多层网络解决异或运
解深度学习的另外一个重要性质一一多层变换
感知机
一个能够模拟异或运算的数据集
##4.2 损失函数定义
神经网络模型的效果以及 优化的目标是通过损失函数（ loss function）来定义
###4.2.1 经典损失函数
分类问题和回归问题是监督学习的两大种类
分类问题希望解决的是将不同的样本分到事先定义好的类别中
一个零件是否合格的问题就是一个二分类问题。
手写体数字识别问题可以被归纳成一个十分类问题。
虽然设置多个阀值在理论上是可能的，但在解决实际问题的过程中一般不会这么处理。 
解决多分类问题最常用的方法是设置 n 个输出节点，其中 n 为类别的个数
判断一个输出向量和期望的向量的接近程度常用房方法：交叉熵（cross entropy）两个概率分布间的距离，常在分类问题中使用p(x)logq(x)
利用Softmax回归将神经网络向前传播得到的结果也变成概率分布。Softmax回归可以作为一个学习算法来优化分类结果，在 TensorFlow 中，它只是一层额外的处理层，将神经网络的输出变成一个概率分布
原始神经网络的输出被用作置信度来生成新的输出，而新的输出满足概率分布的所有要求。这个新的输出可以理解为经过神经网络的推导， 一个样例 为不同类别的概率分别是多大

深层神经网络实际上有组合特征提取的功能
实现交叉熵
```
#实现交叉熵chapter3.4.5
cross_entropy = -tf.reduce_mean(y_*tf.math.log(tf.clip_by_value(y,1e-10,1.0))
```
    tf.clip_by_value可以将张量中的数值限定挂在一个范围内，避免运算错误
    tf.math.log 函数，这个函数完成了对张量中所有元素依次求对数 的功能  
    *，在实现交叉熵的代码中直接将两个矩阵通过“＊”操作相乘。这个 操作不是矩阵乘法，而是元素之间直接相乘。矩阵乘法需要使用 tf.matmul 函数来完成。  
        vl*v2 的结果是每个位置上对应元素的乘积
这三个运算完成了对于每一个样例中的每一个类别交叉熵p(x)logq(x）的计算。这三步计算得到的结果是一个 n × m 的二维矩阵，其中n为一个batch中样例的数量， m 为分类的类别数量。根据交叉熵的公式，应该将每行中的 m 个结果相加得到所有样例的 交叉熵，然后再对这n行取平均得到一个batch的平均交叉熵。但因为分类问题的类别数量是不变的，所以可以直接对整个矩阵做平均而并不改变计算结果的意义。这样的方式可以使整个程序更加简洁。tf.reduce_mean
```python
import tensorflow as tf

v = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
with tf.Session() as sess:
    print(tf.reduce_mean(v).eval())
``` 

因为交叉熵一般会与softmax 回归一起使用，所以 TensorFlow 对这两个功能进行了统一封装，并提供了tf.nn.softmax_cross_entropy_with_logits 函数

在只有一个正确答案的分类问题中，TensorFlow 提供了tf.nn.sparse_softmax_cross_entropy_with_logits,进一步加速运算过程

#####回归问题：与分类问题不同，回归问题解决的是对具体数值的预测
解决回归问题的神经网络一般只有一个输出节点，这个节点的输出值就是预测值。对于回归问题，最常用的损失函数是均方误差 MSE, meansquared error 
```
mse = tf.reduce_mean(tf.square(y_ - y)
```
“-”也是两个矩阵中对应元素的减法。

###4.2.2自定义损失函数
通过自定义损失函数的方法，使得神经网络优化的结果更加接近实际问题的需求。
例：预测商品销量问题
    如果预测多了（预测值比真实销量大），商家损失的是生产商品的 成本；而如果预测少了（预测值比真实销量小〉，损失的则是商品的利润。因为一般商品的成本和商品的利润不会严格相等，所以使用4.2.1 节中介绍的均方误差损失函数就不能够很好地最大化销售利润。注意损失函数定义的是损失，所以要将利润最大化，定义的损失函数应该刻画成本或者代价。 
```
loss= tf.reduce_sum(tf. where(tf.greater(v1，v2) , (v1-v2) * a , (v2-v1) * b))
```
tf.greater的输入是两个张量，此函数会比较这两个输入张量中每一个元素的大小，并返回比较结果。当 tf.greater 的输入张量维度不一样时，TensorFlow会进行类似NumPy广播操作（broadcasting）的处理。①tf.where 函数有三个参数。 第一个为选择条件根据，当选择条件为 True 时， tf.where 函数会选择第二个参数中的值，否则使用第三个参数中的值。注意 tf.where 函数判断和选择都是在元素级别进行，两个序列比较，会将所有元素中最大的输出
```
import tensorflow as tf
v1 = tf.constant([1.0, 2.0, 3.0, 4.0])
v2 = tf.constant([4.0, 3.0, 2.0, 1.0])
sess = tf.compat.v1.InteractiveSession()
print(tf.greater(v1, v2).eval())
# 输出［False False True True]
print(tf.where(tf.greater(v1, v2), v1, v2).eval())
# 输出［4. 3. 3. 4.]
sess.close()
```

##4.3神经网络损失算法
通过反向传播算法（backpropagation）和梯度下降算法 (gradient decent）调整神经网络中参数的取值。
梯度下降算法主要用于优化单个参数的取值
反向传播算法 高效的 在所有参数上使用梯度下降算法，可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值，是训练神经网络的核心算法
神经网络优化过程的基本概念和主要思想
 θ 表示神经网络中的参数， J（θ）表示在给定的参数取值下，训练数据集上损失函数的大小，整个优化过程可以抽象为寻找一个参数θ ，使得 J（θ）最小。梯度下降算法会法代式更新参数 θ ，不断沿着梯度的反方向让参数朝着总损失更小的方向更新
 学习率η（learning rate）来定义每次参数更新的幅度

解释使用梯度下降算法优化参数取值的过程
    神经网络的优化过程可以分为两个阶段，第一个阶段先通过前向传播算法计算得到预测值，井将预测值和真实值做对比得出两者之间的差距。然后在第二个阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数。
缺点：
    梯度下降算法并不能保证被优化的函数达到全局最优解。 
    计算时间太长
计算时长解决：随机梯度下降算法stochastic gradient descent
    不是在全部训练数据上的损失函数，而是在每一轮法代中，随机优化某一条训练数据上的损失函数。这样每一轮参数更新的速度就大大加快了。因为随机梯度下降算法每次优化的只是某一条数据上的损失函数，所以它的问题也非常明显：在某一条数据上损失函数更小并不代表在全部数据上损失函数更小，于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优。 
折中算法：每次计算一小部分训练数据的损失函数，batch
##4.4神经网络进一步优化
神经网络优化过程中可能遇到的问题和解决方法
###4.4.1学习率的设置
通过指数衰减的方法设置梯度下降算法中的学习率。通过指数衰减的学习率既可以让模型在训练的前期快速接近较优解，又可以保证模型在训练后期不会有太大的波动，从而更加接近局部最优。
学习率过大可能不会收敛到一个极小值，过小则降低优化速度，灵活设置学习率：指数衰减法，tf.train.exponential_decay函数实现
###4.4.2过拟合问题
###4.4.3 滑动平均模型
